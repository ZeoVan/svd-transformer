{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a practice notebook for pytorch on image (MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented 4 models:\n",
    "* DNN\n",
    "* CNN\n",
    "* LSTM\n",
    "* Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.optim import SGD,Adam\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "torch.device(\"cpu\");\n",
    "#torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and transform dataset (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the transform function\n",
    "## Convert images to number that understand by pytorch\n",
    "## Separates into 3 colours (RGB)\n",
    "## Converts each pixel to the brightness of its colour (0-255)\n",
    "## Scale the values to a range between 0 and 1\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../Datasets/MNIST/train\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "FloatProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\hazim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# No total? Show info style bar with no progress tqdm status\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                 \u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIProgress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IProgress' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-258cec96ce7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Download dataset and transfrom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrainset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Datasets/MNIST/train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mvalset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Datasets/MNIST/val'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hazim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hazim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmd5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m# process and save as torch files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hazim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m     \u001b[0mdownload_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[0marchive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hazim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5)\u001b[0m\n\u001b[0;32m     83\u001b[0m             urllib.request.urlretrieve(\n\u001b[0;32m     84\u001b[0m                 \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[0mreporthook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgen_bar_updater\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             )\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hazim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36mgen_bar_updater\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgen_bar_updater\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbar_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hazim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         self.container = self.status_printer(\n\u001b[1;32m--> 206\u001b[1;33m             self.fp, total, self.desc, self.ncols)\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hazim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;31m# #187 #451 #558\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             raise ImportError(\n\u001b[1;32m--> 104\u001b[1;33m                 \u001b[1;34m\"FloatProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m                 \u001b[1;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \"/user_install.html\")\n",
      "\u001b[1;31mImportError\u001b[0m: FloatProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "## Download dataset and transfrom\n",
    "\n",
    "trainset = datasets.MNIST('../Datasets/MNIST/train', download=True, train=True, transform=transform)\n",
    "valset = datasets.MNIST('../Datasets/MNIST/val', download=True, train=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "## Load train data\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "## Print shapes of train data (64 images in EACH BATCH and each images has 28x28 pixels)\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAALvElEQVR4nO3dX8hUdR7H8c9nKy8qIctBpD9rG97EglqTbBniEivmjXUTCW3PQmBQQUUXG+1FXcayGisshZlkS1sEFQnJbq4E0UXRJK7Zn93aMFRMH/Eio8i1vnvxnOLJnjnP05xz5kx+3y8YZub85pnzYejTmTm/GX+OCAE4/f2s7QAAhoOyA0lQdiAJyg4kQdmBJM4c5s7mzp0bCxYsGOYugVT27duno0ePeqqxSmW3vUrSnyWdIWlzRDxc9vgFCxao1+tV2SWAEt1ut+/YwG/jbZ8h6S+Srpd0uaS1ti8f9PkANKvKZ/alkj6KiI8j4oSkZyWtqScWgLpVKfuFkvZPun+g2PY9ttfZ7tnujY+PV9gdgCoaPxsfEZsiohsR3U6n0/TuAPRRpewHJV086f5FxTYAI6hK2d+StND2pbZnSbpZ0rZ6YgGo28BTbxFx0vZdkv6hiam3LRHxbm3JANSq0jx7RGyXtL2mLAAaxNdlgSQoO5AEZQeSoOxAEpQdSIKyA0lQdiAJyg4kQdmBJCg7kARlB5Kg7EASlB1IgrIDSVB2IAnKDiRB2YEkKDuQBGUHkqDsQBKUHUhiqEs2Y/Ts37+/dPySSy4pHb/33ntLxzds2PCjM6EZHNmBJCg7kARlB5Kg7EASlB1IgrIDSVB2IAnm2VHKdun45s2bS8dXrlzZd2zVqlUDZcJgKpXd9j5JxyV9LelkRHTrCAWgfnUc2X8dEUdreB4ADeIzO5BE1bKHpFdsv2173VQPsL3Ods92b3x8vOLuAAyqatmvjYgrJF0v6U7by099QERsiohuRHQ7nU7F3QEYVKWyR8TB4vqIpBclLa0jFID6DVx22+fYnv3tbUkrJe2tKxiAelU5Gz9P0ovFPOyZkv4WEX+vJRV+Mo4fP146vn79+r5jy5YtK/3b2bNnD5QJUxu47BHxsaRFNWYB0CCm3oAkKDuQBGUHkqDsQBKUHUiCn7iiUV9++WXfsRMnTgwxCTiyA0lQdiAJyg4kQdmBJCg7kARlB5Kg7EASzLOjUXfccUffsQsuuGCIScCRHUiCsgNJUHYgCcoOJEHZgSQoO5AEZQeSYJ4dpSKidHzJkiWl46tXr64zDirgyA4kQdmBJCg7kARlB5Kg7EASlB1IgrIDSTDPjlLFktx9rVixonT8vPPOqzENqpj2yG57i+0jtvdO2na+7R22Pyyu5zQbE0BVM3kb/6SkVadsu1/SzohYKGlncR/ACJu27BHxmqRjp2xeI2lrcXurpBtqzgWgZoOeoJsXEYeK259KmtfvgbbX2e7Z7o2Pjw+4OwBVVT4bHxO/lOj7a4mI2BQR3YjodjqdqrsDMKBBy37Y9nxJKq6P1BcJQBMGLfs2SWPF7TFJL9UTB0BTpp1nt/2MpBWS5to+IOlBSQ9Les72bZI+kXRTkyHRnI0bN7YdAUMybdkjYm2foetqzgKgQXxdFkiCsgNJUHYgCcoOJEHZgST4ietp7quvviod/+CDD4aUBG3jyA4kQdmBJCg7kARlB5Kg7EASlB1IgrIDSTDPfprbtWtX6fjLL788pCRoG0d2IAnKDiRB2YEkKDuQBGUHkqDsQBKUHUiCefbkJhb0QQYc2YEkKDuQBGUHkqDsQBKUHUiCsgNJUHYgCebZk7PddgQMybRHdttbbB+xvXfStodsH7S9u7isbjYmgKpm8jb+SUmrptj+SEQsLi7b640FoG7Tlj0iXpN0bAhZADSoygm6u2zvKd7mz+n3INvrbPds98bHxyvsDkAVg5b9UUmXSVos6ZCk9f0eGBGbIqIbEd1OpzPg7gBUNVDZI+JwRHwdEd9IelzS0npjAajbQGW3PX/S3Rsl7e33WACjYdp5dtvPSFohaa7tA5IelLTC9mJJIWmfpNsbzAigBtOWPSLWTrH5iQayAGgQX5cFkqDsQBKUHUiCsgNJUHYgCX7iepp76qmnGn3+sbGxRp8f9eHIDiRB2YEkKDuQBGUHkqDsQBKUHUiCsgNJMM9+mtu9e3ejz79o0aJGnx/14cgOJEHZgSQoO5AEZQeSoOxAEpQdSIKyA0kwz36a27BhQ+n4NddcM6QkaBtHdiAJyg4kQdmBJCg7kARlB5Kg7EASlB1Ignn25Gy3HQFDMu2R3fbFtl+1/Z7td23fXWw/3/YO2x8W13OajwtgUDN5G39S0n0RcbmkX0m60/blku6XtDMiFkraWdwHMKKmLXtEHIqIXcXt45Lel3ShpDWSthYP2yrphqZCAqjuR52gs71A0hJJb0qaFxGHiqFPJc3r8zfrbPds98bHxytEBVDFjMtu+1xJz0u6JyI+mzwWESEppvq7iNgUEd2I6HY6nUphAQxuRmW3fZYmiv50RLxQbD5se34xPl/SkWYiAqjDtFNvnpibeULS+xEx+feS2ySNSXq4uH6pkYQYaVdffXXp+MaNG/uOXXXVVXXHQYmZzLMvk/RbSe/Y/vYfIX9AEyV/zvZtkj6RdFMzEQHUYdqyR8Trkvp98+K6euMAaApflwWSoOxAEpQdSIKyA0lQdiAJfuJ6mps1a1bp+Nlnn106/sUXX5SOv/HGG6XjfEV6dHBkB5Kg7EASlB1IgrIDSVB2IAnKDiRB2YEkmGc/zV155ZWl44899ljp+K233lo6Pt2S0MuXLy8dx/BwZAeSoOxAEpQdSIKyA0lQdiAJyg4kQdmBJJhnT+6WW26pNI6fDo7sQBKUHUiCsgNJUHYgCcoOJEHZgSQoO5DEtGW3fbHtV22/Z/td23cX2x+yfdD27uKyuvm4AAY1ky/VnJR0X0Tssj1b0tu2dxRjj0TEn5qLB6AuM1mf/ZCkQ8Xt47bfl3Rh08EA1OtHfWa3vUDSEklvFpvusr3H9hbbc/r8zTrbPds9lgIC2jPjsts+V9Lzku6JiM8kPSrpMkmLNXHkXz/V30XEpojoRkS30+nUEBnAIGZUdttnaaLoT0fEC5IUEYcj4uuI+EbS45KWNhcTQFUzORtvSU9Iej8iNkzaPn/Sw26UtLf+eADqMpOz8csk/VbSO7Z3F9sekLTW9mJJIWmfpNsbSQigFjM5G/+6JE8xtL3+OACawjfogCQoO5AEZQeSoOxAEpQdSIKyA0lQdiAJyg4kQdmBJCg7kARlB5Kg7EASlB1IgrIDSTgihrcze1zSJ5M2zZV0dGgBfpxRzTaquSSyDarObD+PiCn//behlv0HO7d7EdFtLUCJUc02qrkksg1qWNl4Gw8kQdmBJNou+6aW919mVLONai6JbIMaSrZWP7MDGJ62j+wAhoSyA0m0Unbbq2z/2/ZHtu9vI0M/tvfZfqdYhrrXcpYtto/Y3jtp2/m2d9j+sLieco29lrKNxDLeJcuMt/ratb38+dA/s9s+Q9J/JP1G0gFJb0laGxHvDTVIH7b3SepGROtfwLC9XNLnkp6KiF8W2/4o6VhEPFz8j3JORPx+RLI9JOnztpfxLlYrmj95mXFJN0j6nVp87Upy3aQhvG5tHNmXSvooIj6OiBOSnpW0poUcIy8iXpN07JTNayRtLW5v1cR/LEPXJ9tIiIhDEbGruH1c0rfLjLf62pXkGoo2yn6hpP2T7h/QaK33HpJesf227XVth5nCvIg4VNz+VNK8NsNMYdplvIfplGXGR+a1G2T586o4QfdD10bEFZKul3Rn8XZ1JMXEZ7BRmjud0TLewzLFMuPfafO1G3T586raKPtBSRdPun9RsW0kRMTB4vqIpBc1ektRH/52Bd3i+kjLeb4zSst4T7XMuEbgtWtz+fM2yv6WpIW2L7U9S9LNkra1kOMHbJ9TnDiR7XMkrdToLUW9TdJYcXtM0kstZvmeUVnGu98y42r5tWt9+fOIGPpF0mpNnJH/r6Q/tJGhT65fSPpXcXm37WySntHE27r/aeLcxm2SLpC0U9KHkv4p6fwRyvZXSe9I2qOJYs1vKdu1mniLvkfS7uKyuu3XriTXUF43vi4LJMEJOiAJyg4kQdmBJCg7kARlB5Kg7EASlB1I4v9kYY94cHJlbwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Print first image\n",
    "\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r');\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN,self).__init__()\n",
    "        \n",
    "        ## Inputs to hidden layer linear transformation\n",
    "        self.fc1 = nn.Linear(784,128)\n",
    "        ## Fc layer 2\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        ## Output layer, 10 unites - one for each digit\n",
    "        self.output = nn.Linear(64,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ## Pass the input tensor through each operations\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        return(x)\n",
    "    \n",
    "model = DNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1)\n",
    "        self.fc1 = nn.Linear(9216,128)\n",
    "        self.fc2 = nn.Linear(128,10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = torch.flatten(x,start_dim = 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        return(x)\n",
    "\n",
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-short Term Memory class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm1): LSTM(28, 64, num_layers=2, batch_first=True)\n",
      "  (lstm2): LSTM(64, 32, batch_first=True)\n",
      "  (fc1): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=28, \n",
    "                             hidden_size=64, \n",
    "                             num_layers=2,\n",
    "                             batch_first=True) #input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        self.lstm2 = nn.LSTM(input_size=64,\n",
    "                             hidden_size=32,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True)\n",
    "        self.fc1 = nn.Linear(32,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)  -- hidden layer\n",
    "        # h_c shape (n_layers, batch, hidden_size)  -- cell state\n",
    "        # None represents zero initial hidden state\n",
    "        \n",
    "        r_out, (h_n, h_c) = self.lstm1(x,None)\n",
    "        r_out, (h_n, h_c) = self.lstm2(r_out, None)\n",
    "        \n",
    "        x = self.fc1(r_out[:,-1,:])\n",
    "        return(x)\n",
    "\n",
    "model = LSTM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Long-short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (lstm1): LSTM(28, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc1): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM,self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=28,\n",
    "                             hidden_size=64,\n",
    "                             num_layers=2,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=True)\n",
    "        self.fc1 = nn.Linear(64*2,10)   ## 2 for bidirectional\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        r_out, (h_n, h_c) = self.lstm1(x,None)\n",
    "        x = self.fc1(r_out[:,-1,:])\n",
    "        return(x)\n",
    "\n",
    "model = BiLSTM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN & TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing for LSTM (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 2.300307 - Time: 47.8980s\n",
      "Epoch 2 - Training loss: 2.292306 - Time: 48.4485s\n",
      "Epoch 3 - Training loss: 2.157634 - Time: 48.6506s\n",
      "Epoch 4 - Training loss: 1.239763 - Time: 46.7079s\n",
      "Epoch 5 - Training loss: 0.454633 - Time: 48.6574s\n"
     ]
    }
   ],
   "source": [
    "############### TRAINING for LSTM\n",
    "\n",
    "## Define optimizer\n",
    "optimizer = SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "#optimizer = Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "## Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "## Training starts here\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    timer = time.time()\n",
    "    for step,(x,y) in enumerate(trainloader):\n",
    "\n",
    "        ## reshape images to (batch, time_step, input_size)(LSTM)\n",
    "        b_x = Variable(x.view(-1, 28, 28))              # reshape x to (batch, time_step, input_size)\n",
    "        b_y = Variable(y)                               # batch y\n",
    "        \n",
    "        ## For each iteration reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## Run the input data through the network (forward pass)\n",
    "        output = model(b_x)\n",
    "        \n",
    "        ## Calculate the losses using the loss functioon\n",
    "        loss = criterion(output, b_y)\n",
    "        \n",
    "        ## Perform backpropagation \n",
    "        loss.backward()\n",
    "        \n",
    "        ## Updates the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {:.6f} - Time: {:.4f}s\".format(e+1, running_loss/len(trainloader), (time.time()-timer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### TESTING for LSTM\n",
    "\n",
    "all_labels = []\n",
    "probs = pd.Series([0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, (x,y) in enumerate(valloader):\n",
    "        ## reshape images to (batch, time_step, input_size)(LSTM)\n",
    "        b_x = Variable(x.view(-1, 28, 28))              # reshape x to (batch, time_step, input_size)\n",
    "        b_y = Variable(y)                               # batch y\n",
    "\n",
    "        output_test = model(b_x)\n",
    "        output_test = torch.exp(output_test)\n",
    "        output_test = output_test.tolist()\n",
    "        output_test = pd.Series(output_test)\n",
    "        probs = probs.append(output_test,ignore_index=True)\n",
    "        all_labels += b_y.tolist()\n",
    "\n",
    "def getClass(x):\n",
    "    return(x.index(max(x)))\n",
    "\n",
    "all_predicted = probs[1:].apply(getClass)\n",
    "all_predicted.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function (DNN,CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define optimizer\n",
    "optimizer = SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "#optimizer = Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "## Define loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "## Training starts here\n",
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    timer = time.time()\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        ## Flatten the images into 784 long vector (DNN)\n",
    "        #images = images.view(images.shape[0], -1)\n",
    "\n",
    "        ## For each iteration reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## Run the input data through the network (forward pass)\n",
    "        output = model(images)\n",
    "        \n",
    "        ## Calculate the losses using the loss functioon\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        ## Perform backpropagation \n",
    "        loss.backward()\n",
    "        \n",
    "        ## Updates the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {:.6f} - Time: {:.4f}s\".format(e+1, running_loss/len(trainloader), (time.time()-timer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing (DNN,CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "probs = pd.Series([0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in valloader:\n",
    "            \n",
    "            # For DNN\n",
    "            #images = images.view(images.shape[0], -1)\n",
    "            \n",
    "            output_test = model(images)\n",
    "            output_test = torch.exp(output_test)\n",
    "            output_test = output_test.tolist()\n",
    "            output_test = pd.Series(output_test)\n",
    "            probs = probs.append(output_test,ignore_index=True)\n",
    "            all_labels += labels.tolist()\n",
    "\n",
    "def getClass(x):\n",
    "    return(x.index(max(x)))\n",
    "\n",
    "all_predicted = probs[1:].apply(getClass)\n",
    "all_predicted.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (ALL models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy:  0.9206\n",
      "\n",
      "[[ 945    0    7    2    1   10    3    1   11    0]\n",
      " [   0 1119    8    1    0    4    0    0    2    1]\n",
      " [   6    2  970   36    0   11    0    6    0    1]\n",
      " [   0    1   30  904    0   39    0   28    1    7]\n",
      " [   0    0    4    0  852    3   14    2    5  102]\n",
      " [   4    2    7   43    1  808    0    7    6   14]\n",
      " [  20    4   13    0   12   42  854    0   13    0]\n",
      " [   0    9   10   15    3    6    0  956    0   29]\n",
      " [  24    4    4   11    1   52    3    0  867    8]\n",
      " [   1    1    0    3   17   25    0   29    2  931]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "acc = (pd.value_counts(all_predicted == all_labels)[1])/len(all_labels)\n",
    "print('Testing accuracy: ', acc)\n",
    "print()\n",
    "\n",
    "confusion = sklearn.metrics.confusion_matrix(y_true=all_labels, y_pred=all_predicted)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
