{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a practice notebook for pytorch (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6ec0aef230>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1234\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and transform dataset (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the transform function\n",
    "## Convert images to number that understand by pytorch\n",
    "## Separates into 3 colours (RGB)\n",
    "## Converts each pixel to the brightness of its colour (0-255)\n",
    "## Scale the values to a range between 0 and 1\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download dataset and transfrom\n",
    "\n",
    "trainset = datasets.MNIST('../Datasets/MNIST/train', download=True, train=True, transform=transform)\n",
    "valset = datasets.MNIST('../Datasets/MNIST/val', download=True, train=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "## Load train data\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "## Print shapes of train data (64 images in EACH BATCH and each images has 28x28 pixels)\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Print first image\n",
    "\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r');\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        ## Inputs to hidden layer linear transformation\n",
    "        self.fc1 = nn.Linear(784,128)\n",
    "        ## Fc layer 2\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        ## Output layer, 10 unites - one for each digit\n",
    "        self.output = nn.Linear(64,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ## Pass the input tensor through each operations\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        \n",
    "        return(x)\n",
    "    \n",
    "model = DNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.06839747598984523 - Time: 6.904\n",
      "Epoch 1 - Training loss: 0.06351189726074018 - Time: 13.980\n",
      "Epoch 2 - Training loss: 0.059363766724783884 - Time: 20.995\n",
      "Epoch 3 - Training loss: 0.05547568886784086 - Time: 27.979\n",
      "Epoch 4 - Training loss: 0.05215928316993047 - Time: 34.897\n",
      "Epoch 5 - Training loss: 0.049272442286683205 - Time: 41.892\n",
      "Epoch 6 - Training loss: 0.046396722273676715 - Time: 48.984\n",
      "Epoch 7 - Training loss: 0.043560585269688576 - Time: 56.018\n",
      "Epoch 8 - Training loss: 0.04081118111955379 - Time: 63.171\n",
      "Epoch 9 - Training loss: 0.038800192171030604 - Time: 70.027\n",
      "Epoch 10 - Training loss: 0.036530200935982624 - Time: 77.191\n",
      "Epoch 11 - Training loss: 0.03398177728564766 - Time: 84.313\n",
      "Epoch 12 - Training loss: 0.03261974094167991 - Time: 91.463\n",
      "Epoch 13 - Training loss: 0.03040728509078112 - Time: 98.487\n",
      "Epoch 14 - Training loss: 0.029746444451797412 - Time: 105.596\n"
     ]
    }
   ],
   "source": [
    "## Define optimizer\n",
    "optimizer = SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "\n",
    "## Define loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "## Training starts here\n",
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    timer = time.time()\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        ## Flatten the images into 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        ## For each iteration reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## Run the input data through the network (forward pass)\n",
    "        output = model(images)\n",
    "        \n",
    "        ## Calculate the losses using the loss functioon\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        ## Perform backpropagation \n",
    "        loss.backward()\n",
    "        \n",
    "        ## Updates the weigh\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {} - Time: {:.4f}s\".format(e, running_loss/len(trainloader), (time.time()-timer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "probs = pd.Series([0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in valloader:\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            output_test = model(images)\n",
    "            output_test = torch.exp(output_test)\n",
    "            output_test = output_test.tolist()\n",
    "            output_test = pd.Series(output_test)\n",
    "            probs = probs.append(output_test,ignore_index=True)\n",
    "            all_labels += labels.tolist()\n",
    "\n",
    "def getClass(x):\n",
    "    return(x.index(max(x)))\n",
    "\n",
    "all_predicted = probs[1:].apply(getClass)\n",
    "all_predicted.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy:  0.9769\n",
      "\n",
      "[[ 968    0    0    1    1    3    4    1    2    0]\n",
      " [   0 1127    2    0    0    1    2    0    3    0]\n",
      " [   5    1 1007    1    3    0    4    5    5    1]\n",
      " [   3    0    2  991    0    2    0    6    3    3]\n",
      " [   3    0    4    0  962    0    2    3    0    8]\n",
      " [   3    1    0    9    3  863    5    0    7    1]\n",
      " [   3    3    2    1    6    3  936    0    4    0]\n",
      " [   1    7    7    2    2    1    0  993    4   11]\n",
      " [   4    0    2    6    3    4    3    2  947    3]\n",
      " [   1    3    0    6   11    1    1    7    4  975]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "acc = (pd.value_counts(all_predicted == all_labels)[1])/len(all_labels)\n",
    "print('Testing accuracy: ', acc)\n",
    "print()\n",
    "\n",
    "confusion = sklearn.metrics.confusion_matrix(y_true=all_labels, y_pred=all_predicted)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
