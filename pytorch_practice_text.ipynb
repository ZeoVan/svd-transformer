{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a practice notebook for pytorch on text (IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented 4 models:\n",
    "* RNN\n",
    "* LSTM\n",
    "* CNN\n",
    "* Stacked CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.optim import SGD,Adam\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "#torch.device(\"cpu\");\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## batch_first for CNN\n",
    "\n",
    "TEXT = torchtext.data.Field(tokenize = 'spacy',batch_first=True)\n",
    "LABEL = torchtext.data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = torchtext.datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train)}')\n",
    "print(f'Number of testing examples: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train.split(random_state = random.seed(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train)}')\n",
    "print(f'Number of validation examples: {len(valid)}')\n",
    "print(f'Number of testing examples: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the vocabulary\n",
    "MAX_VOCAB_SIZE = 8000\n",
    "\n",
    "TEXT.build_vocab(train, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 8002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 203566), (',', 192495), ('.', 165618), ('and', 109442), ('a', 109116), ('of', 100702), ('to', 93766), ('is', 76328), ('in', 61255), ('I', 54004), ('it', 53508), ('that', 49187), ('\"', 44282), (\"'s\", 43329), ('this', 42445), ('-', 36692), ('/><br', 35752), ('was', 35034), ('as', 30384), ('with', 29774)]\n"
     ]
    }
   ],
   "source": [
    "## Most common word\n",
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n",
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## place into iterators\n",
    "train_iterator, valid_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "    (train, valid, test), \n",
    "    batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8002"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embed): Embedding(52, 20)\n",
      "  (rnn1): RNN(20, 10)\n",
      "  (fc1): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN,self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=52,\n",
    "                                  embedding_dim=20)\n",
    "        self.rnn1 = nn.RNN(input_size=20,\n",
    "                      hidden_size=10,\n",
    "                      num_layers=1)\n",
    "        self.fc1  = nn.Linear(10,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        #embed = [sent len, batch size, emb dim]\n",
    "        x = self.embed(x)\n",
    "        rnn_out, hidden = self.rnn1(x,None)\n",
    "        x = rnn_out[-1,:,:]\n",
    "        x = self.fc1(x.squeeze(0))\n",
    "        x = F.log_softmax(x)\n",
    "        return(x)\n",
    "\n",
    "model = RNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (embed): Embedding(52, 20)\n",
      "  (lstm): LSTM(20, 20)\n",
      "  (fc): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=52,\n",
    "                                  embedding_dim=20)\n",
    "        self.lstm = nn.LSTM(input_size=20,\n",
    "                            hidden_size=20,\n",
    "                            num_layers=1)\n",
    "        self.fc = nn.Linear(20,1)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        embed = self.embed(x)\n",
    "        lstm_out, hidden = self.lstm(embed,None)\n",
    "        x = lstm_out[-1,:,:]\n",
    "        x = self.fc(x.squeeze(1))\n",
    "        return(x)\n",
    "\n",
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Net (Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (embed): Embedding(8002, 100)\n",
      "  (CNN): Conv2d(1, 100, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=80, out_features=30, bias=True)\n",
      "  (fc2): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=8002,\n",
    "                                  embedding_dim=100)\n",
    "        self.CNN = nn.Conv2d(in_channels=1,\n",
    "                             out_channels=100,\n",
    "                             kernel_size=3)\n",
    "        self.fc1 = nn.Linear(80,30)\n",
    "        self.fc2 = nn.Linear(30,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.embed(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.CNN(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        k = x.shape\n",
    "        x = torch.flatten(x,start_dim = 1)\n",
    "        self.fc1 = nn.Linear(k[1]*k[2]*k[3],30)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x.squeeze(0))\n",
    "        x = F.log_softmax(x)\n",
    "        return(x)\n",
    "\n",
    "model = CNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Net (Stack 3 Convs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=52,\n",
    "                                  embedding_dim=30)\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1,\n",
    "                              out_channels=20,\n",
    "                              kernel_size=2)\n",
    "        self.cnn2 = nn.Conv2d(in_channels=1,\n",
    "                              out_channels=20,\n",
    "                              kernel_size=3)\n",
    "        self.cnn3 = nn.Conv2d(in_channels=1,\n",
    "                              out_channels=20,\n",
    "                              kernel_size=4)\n",
    "        self.fc1 = nn.Linear(60,30)\n",
    "        self.fc2 = nn.Linear(30,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.embed(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        x1 = F.relu(self.cnn1(x))\n",
    "        x1 = F.max_pool2d(x1,2)\n",
    "        x1 = torch.flatten(x1,start_dim=1)\n",
    "        \n",
    "        x2 = F.relu(self.cnn2(x))\n",
    "        x2 = F.max_pool2d(x2,3)\n",
    "        x2 = torch.flatten(x2,start_dim=1)\n",
    "        \n",
    "        x3 = F.relu(self.cnn3(x))\n",
    "        x3 = F.max_pool2d(x3,4)\n",
    "        x3 = torch.flatten(x3,start_dim=1)\n",
    "        \n",
    "        x = torch.cat((x1,x2,x3), dim=1)\n",
    "        self.fc1 = nn.Linear(x.shape[1],30)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x.squeeze(0))\n",
    "        x = F.log_softmax(x)\n",
    "        return(x)\n",
    "\n",
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN & TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 803,692 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_accuracy(probs,all_labels):\n",
    "    def getClass(x):\n",
    "        return(x.index(max(x)))\n",
    "\n",
    "    all_predicted = probs.apply(getClass)\n",
    "    all_predicted.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    acc = (pd.value_counts(all_predicted == all_labels)[1])/len(all_labels)\n",
    "    return(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Embeddings (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# Then zero the initial weights of the unknown and padding tokens.\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### TRAINING for LSTM\n",
    "\n",
    "## Define optimizer\n",
    "optimizer = SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "#optimizer = Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "## Define loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "## Training starts here\n",
    "epochs = 5\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    timer = time.time()\n",
    "    for batch in train_iterator:\n",
    "\n",
    "        ## For each iteration reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## Run the input data through the network (forward pass)\n",
    "        output = model(batch.text).squeeze(1)\n",
    "        \n",
    "        ## Calculate the losses using the loss functioon\n",
    "        loss = criterion(output,batch.label)\n",
    "        \n",
    "        ## Binary acc\n",
    "        acc = binary_accuracy(output, batch.label)\n",
    "        \n",
    "        ## Perform backpropagation \n",
    "        loss.backward()\n",
    "        \n",
    "        ## Updates the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_acc += acc.item()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            running_loss_val = 0\n",
    "            running_acc_val = 0\n",
    "            for batch_val in valid_iterator:\n",
    "\n",
    "                ## Run the input data through the network (forward pass)\n",
    "                output_val = model(batch_val.text).squeeze(1)\n",
    "\n",
    "                ## Binary acc\n",
    "                acc_val = binary_accuracy(output_val, batch_val.label)\n",
    "                \n",
    "                ## Calculate the losses using the loss functioon\n",
    "                loss_val = criterion(output_val,batch_val.label)\n",
    "\n",
    "                running_loss_val += loss_val.item()\n",
    "                running_acc_val += acc_val.item()\n",
    "            \n",
    "        \n",
    "        print(\"Epoch {} - Training acc: {:.6f} -Training loss: {:.6f} - Val acc: {:.6f} - Val loss: {:.6f} - Time: {:.4f}s\".format(e+1, running_acc/len(train_iterator), running_loss/len(train_iterator), running_acc_val/len(valid_iterator), running_loss_val/len(valid_iterator), (time.time()-timer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 8.00 GiB total capacity; 5.58 GiB already allocated; 402.77 MiB free; 5.60 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-bdbd8f27fa90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m## Run the input data through the network (forward pass)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m## Calculate the losses using the loss functioon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hazim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-471c8906f226>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstart_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hazim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hazim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[0mstride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     return torch.max_pool2d(\n\u001b[1;32m--> 488\u001b[1;33m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m max_pool2d = boolean_dispatch(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 8.00 GiB total capacity; 5.58 GiB already allocated; 402.77 MiB free; 5.60 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "############### TRAINING for RNN using SOFTMAX\n",
    "\n",
    "## Define optimizer\n",
    "optimizer = SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "#optimizer = Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "## Define loss function\n",
    "criterion = nn.NLLLoss().to(device)\n",
    "\n",
    "## Training starts here\n",
    "epochs = 5\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    all_labels = []\n",
    "    timer = time.time()\n",
    "    probs = pd.Series()\n",
    "    for batch in train_iterator:\n",
    "\n",
    "        ## For each iteration reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## Run the input data through the network (forward pass)\n",
    "        output = model(batch.text.to(device))\n",
    "        \n",
    "        ## Calculate the losses using the loss functioon\n",
    "        loss = criterion(output,batch.label.type(torch.LongTensor).to(device))\n",
    "        \n",
    "        ## Softmax acc\n",
    "        output = torch.exp(output)\n",
    "        output = output.tolist()\n",
    "        probs = probs.append(pd.Series(output),ignore_index=True)\n",
    "        \n",
    "        ## Perform backpropagation \n",
    "        loss.backward()\n",
    "        \n",
    "        ## Updates the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        all_labels += batch.label.tolist()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            running_loss_val = 0\n",
    "            all_labels_val = []\n",
    "            probs_val = pd.Series()\n",
    "            for batch_val in valid_iterator:\n",
    "\n",
    "                ## Run the input data through the network (forward pass)\n",
    "                output_val = model(batch_val.text.to(device))\n",
    "\n",
    "                ## Calculate the losses using the loss functioon\n",
    "                loss_val = criterion(output_val,batch_val.label.type(torch.LongTensor).to(device))\n",
    "                \n",
    "                ## Softmax acc\n",
    "                output_val = torch.exp(output_val)\n",
    "                output_val = output_val.tolist()\n",
    "                probs_val = probs_val.append(pd.Series(output_val),ignore_index=True)\n",
    "\n",
    "                running_loss_val += loss_val.item()\n",
    "                all_labels_val += batch_val.label.tolist()\n",
    "            \n",
    "        \n",
    "        print(\"Epoch {} - Training acc: {:.6f} -Training loss: {:.6f} - Val acc: {:.6f} - Val loss: {:.6f} - Time: {:.4f}s\".format(e+1, softmax_accuracy(probs,all_labels), running_loss/len(train_iterator), softmax_accuracy(probs_val,all_labels_val), running_loss_val/len(valid_iterator), (time.time()-timer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For binary\n",
    "\n",
    "with torch.no_grad():\n",
    "    running_loss_test=0\n",
    "    running_acc_test=0\n",
    "    for batch in test_iterator:\n",
    "\n",
    "        output_test = model(batch.text).squeeze(1)\n",
    "        acc_test = binary_accuracy(output_test, batch.label)\n",
    "        loss_test = criterion(output_test,batch.label)\n",
    "        running_loss_test += loss_test.item()\n",
    "        running_acc_test += acc_test.item()\n",
    "\n",
    "    print('Test acc: ',running_acc_test/len(test_iterator))\n",
    "    print('Test loss: ',running_loss_test/len(test_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For SOFTMAX\n",
    "with torch.no_grad():\n",
    "    running_loss_test=0\n",
    "    all_labels_test = []\n",
    "    probs_test = pd.Series()\n",
    "    for batch in test_iterator:\n",
    "\n",
    "                ## Run the input data through the network (forward pass)\n",
    "                output_test = model(batch.text)\n",
    "\n",
    "                ## Calculate the losses using the loss functioon\n",
    "                loss_test = criterion(output_test,batch.label.type(torch.LongTensor))\n",
    "                \n",
    "                ## Softmax acc\n",
    "                output_test = torch.exp(output_test)\n",
    "                output_test = output_test.tolist()\n",
    "                probs_test = probs_test.append(pd.Series(output_test),ignore_index=True)\n",
    "\n",
    "                running_loss_test += loss_test.item()\n",
    "                all_labels_test += batch.label.tolist()\n",
    "\n",
    "    print('Test acc: ', softmax_accuracy(probs_test,all_labels_test))\n",
    "    print('Test loss: ',running_loss_test/len(test_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
