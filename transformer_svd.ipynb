{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground for Transformers!\n",
    "\n",
    "### Attention is all you need \n",
    "(https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a minimal example of this **CRAZY** idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torchtext.vocab as vocab\n",
    "import sklearn.metrics\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.optim import SGD,Adam\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from clang import cindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device('cpu')\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "cudnn.benchmark = True\n",
    "cudnn.enabled = True\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load playset dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle('playset(0.25.2).pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>functionSource</th>\n",
       "      <th>combine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93792</th>\n",
       "      <td>go_file_opener_open (GOFileOpener const *fo, g...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79770</th>\n",
       "      <td>updatePathMap(bool left_level) {\\n\\tPoint from...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66999</th>\n",
       "      <td>interpret_tilde(const char* path) {\\n    stati...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44284</th>\n",
       "      <td>checkVarExp(\\n        Absyn *node,\\n        Ta...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49515</th>\n",
       "      <td>will_have_skip_worktree(const struct cache_ent...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96701</th>\n",
       "      <td>AVLTree_insert(AVLTree * tree, void * data)\\n{...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67815</th>\n",
       "      <td>remove_hook(const char *name, hookfn fn)\\n{\\n\\...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88363</th>\n",
       "      <td>output_def(dico_stream_t str, struct gcide_db ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65929</th>\n",
       "      <td>getState(\\n\\t\\tFLMUINT\\t\\tuiFieldID)\\n\\t{\\n\\t\\...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16426</th>\n",
       "      <td>untag_proplist(Pulse_Tag *tag, Eina_Hash **pro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          functionSource  combine\n",
       "93792  go_file_opener_open (GOFileOpener const *fo, g...    False\n",
       "79770  updatePathMap(bool left_level) {\\n\\tPoint from...    False\n",
       "66999  interpret_tilde(const char* path) {\\n    stati...    False\n",
       "44284  checkVarExp(\\n        Absyn *node,\\n        Ta...     True\n",
       "49515  will_have_skip_worktree(const struct cache_ent...     True\n",
       "...                                                  ...      ...\n",
       "96701  AVLTree_insert(AVLTree * tree, void * data)\\n{...    False\n",
       "67815  remove_hook(const char *name, hookfn fn)\\n{\\n\\...    False\n",
       "88363  output_def(dico_stream_t str, struct gcide_db ...    False\n",
       "65929  getState(\\n\\t\\tFLMUINT\\t\\tuiFieldID)\\n\\t{\\n\\t\\...    False\n",
       "16426  untag_proplist(Pulse_Tag *tag, Eina_Hash **pro...     True\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>functionSource</th>\n",
       "      <th>combine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>100000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>intersect(const Rect&amp; rect) {\\n    RectList rl...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           functionSource combine\n",
       "count                                              100000  100000\n",
       "unique                                             100000       2\n",
       "top     intersect(const Rect& rect) {\\n    RectList rl...    True\n",
       "freq                                                    1   50000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CardPowerOff (reader* globalData, char socket) {\\r\\n    char cmd[4], ack;\\r\\n    int retVal, actual, retryTimes = 2;\\r\\n\\r\\n#ifdef ASE_DEBUG\\r\\n    syslog(LOG_INFO, \"\\\\n CardPowerOff - Enter\\\\n\");\\r\\n#endif\\r\\n\\r\\n    if ((retVal = cardCommandInit(globalData, socket, 1)))\\r\\n        return retVal;\\r\\n\\r\\n    cmd[0] = ASE_PACKET_TYPE(0x50, globalData->commandCounter, socket);\\r\\n    globalData->commandCounter++;\\r\\n    globalData->commandCounter %= 4;\\r\\n    cmd[1] = 0x21;\\r\\n    cmd[2] = 0x0;\\r\\n    cmd[3] = cmd[0] ^ cmd[1] ^ cmd[2];\\r\\n\\r\\n    do {\\r\\n        lock_mutex(globalData);\\r\\n        retVal = sendControlCommand(globalData, socket, cmd, 4, &ack, &actual, 0);\\r\\n        unlock_mutex(globalData);\\r\\n\\r\\n        retryTimes--;\\r\\n    } while (retVal != ASE_OK && retryTimes);\\r\\n\\r\\n    // if during the 3 tries the command failed, return an error status\\r\\n    if (retVal < 0) {\\r\\n        return retVal; \\r\\n    }\\r\\n\\r\\n    if (ack != 0x20) {\\r\\n        return parseStatus(ack); \\r\\n    }\\r\\n\\r\\n    /* if the card is present, change the status to powered off */\\r\\n    if (globalData->cards[(int)socket].status)\\r\\n\\t    globalData->cards[(int)socket].status = 1; \\r\\n\\r\\n#ifdef ASE_DEBUG\\r\\n    syslog(LOG_INFO, \" CardPowerOff - Exit\\\\n\");\\r\\n#endif\\r\\n\\r\\n    return ASE_OK; \\r\\n}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.functionSource[24492]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clang Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check your library path for clang llvm\n",
    "cindex.Config.set_library_path('/usr/lib/llvm-6.0/lib')\n",
    "idx = cindex.Index.create()\n",
    "global c\n",
    "c=0\n",
    "def clang_tokenizer(code):\n",
    "    global c\n",
    "    c+=1\n",
    "    print('{0}'.format(c), end='\\r')\n",
    "    \n",
    "    ## Remove code comments\n",
    "    pat = re.compile(r'(/\\*([^*]|(\\*+[^*/]))*\\*+/)|(//.*)')\n",
    "    code = re.sub(pat,'',code)\n",
    "    \n",
    "    ## Tokkenize using clang\n",
    "    tok = []\n",
    "    tu = idx.parse('tmp.cpp',\n",
    "                   args=[''],  \n",
    "                   unsaved_files=[('tmp.cpp', code)],  \n",
    "                   options=0)\n",
    "    for t in tu.get_tokens(extent=tu.cursor.extent):\n",
    "        tok.append(t.spelling)\n",
    "    return(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.functionSource.apply(clang_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaner & Tokkenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global c\n",
    "c=0\n",
    "def my_tokenizer(code):\n",
    "    global c\n",
    "    c+=1\n",
    "    print('{0}'.format(c), end='\\r')\n",
    "    \n",
    "    ## Remove code comments\n",
    "    pat = re.compile(r'(/\\*([^*]|(\\*+[^*/]))*\\*+/)|(//.*)')\n",
    "    code = re.sub(pat,'',code)\n",
    "\n",
    "    ## Remove newlines & tabs\n",
    "    code = re.sub('(\\n)|(\\\\\\\\n)|(\\\\\\\\)|(\\\\t)|(/)|(\\\\r)','',code)\n",
    "    \n",
    "    ## Mix split (characters and words)\n",
    "    splitter = '\\\"(.*?)\\\"| +|(;)|(\\()|(==)|(!=)|(<=)|(>=)|(\\+\\+)|(--)|(\\))|(=)|(\\+)|(\\-)|(\\[)|(\\])|(<)|(>)|(\\.)|({)'\n",
    "    code = re.split(splitter,code)\n",
    "    \n",
    "    ## Remove None type\n",
    "    code = list(filter(None, code))\n",
    "    \n",
    "    code = list(filter(str.strip, code))\n",
    "    #code = \" \".join(code)\n",
    "    \n",
    "    ## Return list of tokens\n",
    "    return(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\r"
     ]
    }
   ],
   "source": [
    " dataset.functionSource = dataset.functionSource.apply(my_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data (in JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change labels boolean to 1 and 0\n",
    "dataset.iloc[:,1] = np.multiply(dataset.iloc[:,1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change column name\n",
    "dataset = dataset.rename(columns={'functionSource':'codes', 'combine':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "false = dataset[dataset.iloc[:,1]==0]\n",
    "true = dataset[dataset.iloc[:,1]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split to train,test,valid\n",
    "train = false[0:20000].append(true[0:1333])\n",
    "test  = false[20000:22494].append(true[2000:2166])\n",
    "valid = false[30000:32494].append(true[3000:3166])\n",
    "\n",
    "## Shuffle\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "test = test.sample(frac=1).reset_index(drop=True)\n",
    "valid = valid.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to json\n",
    "train.to_json('.data/train_ratio_mytoken.json', orient='records',lines=True)\n",
    "test.to_json('.data/test_ratio_mytoken.json', orient='records',lines=True)\n",
    "valid.to_json('.data/valid_ratio_mytoken.json', orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the field CLANG\n",
    "\n",
    "CODES = torchtext.data.Field(batch_first=True, tokenize=None)\n",
    "LABEL = torchtext.data.LabelField(dtype=torch.long)\n",
    "fields = {'codes': ('codes', CODES), 'label': ('label', LABEL)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import ratio CLANG dataset 20K\n",
    "train_data, valid_data, test_data = torchtext.data.TabularDataset.splits(\n",
    "                                        path = '.data',\n",
    "                                        train = 'train_ratio_clang.json',\n",
    "                                        validation = 'valid_ratio_clang.json',\n",
    "                                        test = 'test_ratio_clang.json',\n",
    "                                        format = 'json',\n",
    "                                        fields = fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the field MYTOKEN\n",
    "\n",
    "CODES = torchtext.data.Field(batch_first=True, tokenize=my_tokenizer)\n",
    "LABEL = torchtext.data.LabelField(dtype=torch.long)\n",
    "fields = {'codes': ('codes', CODES), 'label': ('label', LABEL)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import ratio mytoken dataset 20K\n",
    "train_data, valid_data, test_data = torchtext.data.TabularDataset.splits(\n",
    "                                        path = '.data',\n",
    "                                        train = 'train_ratio_mytoken.json',\n",
    "                                        validation = 'valid_ratio_mytoken.json',\n",
    "                                        test = 'test_ratio_mytoken.json',\n",
    "                                        format = 'json',\n",
    "                                        fields = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'codes': ['__modf', '(', 'double', 'x,', 'double', '*iptr', ')', '{', 'int32_t', 'i0,', 'i1,', 'j0', ';', 'u_int32_t', 'i', ';', 'EXTRACT_WORDS', '(', 'i0,', 'i1,', 'x', ')', ';', 'j0', '=', '(', '(', 'i0', '>', '>', '20', ')', '&', '0x7ff', ')', '-', '0x3ff', ';', 'if', '(', 'j0', '<', '20', ')', '{', 'if', '(', 'j0', '<', '0', ')', '{', 'INSERT_WORDS', '(', '*iptr,', 'i0', '&', '0x80000000,', '0', ')', ';', 'return', 'x', ';', '}', 'else', '{', 'i', '=', '(', '0x000fffff', ')', '>', '>', 'j0', ';', 'if', '(', '(', '(', 'i0', '&', 'i', ')', '|', 'i1', ')', '==', '0', ')', '{', '*iptr', '=', 'x', ';', 'INSERT_WORDS', '(', 'x,', 'i0', '&', '0x80000000,', '0', ')', ';', 'return', 'x', ';', '}', 'else', '{', 'INSERT_WORDS', '(', '*iptr,', 'i0', '&', '(', '~i', ')', ',', '0', ')', ';', 'return', 'x', '-', '*iptr', ';', '}}', '}', 'else', 'if', '(', '__builtin_expect', '(', 'j0', '>', '51,', '0', ')', ')', '{', '*iptr', '=', 'x', '*', 'one', ';', 'if', '(', 'j0', '==', '0x400', '&&', '(', '(', 'i0', '&', '0xfffff', ')', '|', 'i1', ')', ')', 'return', 'x', '*', 'one', ';', 'INSERT_WORDS', '(', 'x,', 'i0', '&', '0x80000000,', '0', ')', ';', 'return', 'x', ';', '}', 'else', '{', 'i', '=', '(', '(', 'u_int32_t', ')', '(', '0xffffffff', ')', ')', '>', '>', '(', 'j0', '-', '20', ')', ';', 'if', '(', '(', 'i1', '&', 'i', ')', '==', '0', ')', '{', '*iptr', '=', 'x', ';', 'INSERT_WORDS', '(', 'x,', 'i0', '&', '0x80000000,', '0', ')', ';', 'return', 'x', ';', '}', 'else', '{', 'INSERT_WORDS', '(', '*iptr,', 'i0,', 'i1', '&', '(', '~i', ')', ')', ';', 'return', 'x', '-', '*iptr', ';', '}', '}}'], 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "print(vars(valid_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary-related preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Build the vocabulary\n",
    "\n",
    "MAX_VOCAB_SIZE = 8000\n",
    "\n",
    "CODES.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 8002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(CODES.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('(', 300118), (')', 300016), (';', 278344), (',', 173407), ('=', 111668), ('->', 104361), ('*', 79116), ('}', 69999), ('{', 69973), ('if', 59159), ('0', 39408), ('.', 38924), ('return', 33917), ('[', 31029), (']', 31029), ('&', 27243), ('i', 27084), ('1', 21986), ('==', 20336), ('int', 19798), ('NULL', 18688), (':', 14006), ('-', 14000), ('<', 12865), ('!', 12818), ('else', 12710), ('struct', 12587), ('+', 12098), ('!=', 11578), ('char', 11404), ('++', 11076), ('const', 10973), ('::', 8833), ('&&', 8765), ('case', 8298), ('p', 7546), ('for', 7545), ('break', 7294), ('>', 6727), ('ret', 6282), ('data', 6202), ('||', 5805), ('2', 5701), ('s', 5594), ('<<', 5363), ('#', 5356), ('unsigned', 4986), ('name', 4707), ('len', 4556), ('buf', 4542), ('sizeof', 4480), ('n', 4408), ('x', 4186), ('c', 4029), ('void', 3969), ('size', 3895), ('j', 3826), ('priv', 3749), ('+=', 3729), ('result', 3469), ('dev', 3429), ('this', 3371), ('type', 3268), ('while', 3257), ('value', 3209), ('goto', 3052), ('false', 2786), ('?', 2771), ('error', 2771), ('3', 2626), ('self', 2611), ('FALSE', 2528), ('y', 2527), ('t', 2494), ('r', 2473), ('>=', 2470), ('err', 2470), ('/', 2463), ('flags', 2420), ('4', 2375), ('next', 2361), ('state', 2312), ('double', 2304), ('true', 2303), ('str', 2253), ('d', 2145), ('long', 2137), ('val', 2127), ('endif', 2127), ('|', 2098), ('count', 2024), ('status', 1979), ('res', 1974), ('a', 1972), ('f', 1972), ('ctx', 1962), ('list', 1955), ('l', 1932), ('buffer', 1926), ('size_t', 1905)]\n"
     ]
    }
   ],
   "source": [
    "## Most common word\n",
    "print(CODES.vocab.freqs.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '(', ')', ';', ',', '=', '->', '*', '}']\n",
      "defaultdict(None, {0: 0, 1: 1})\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(CODES.vocab.itos[:10])\n",
    "print(LABEL.vocab.stoi)\n",
    "print(CODES.vocab.stoi[CODES.pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## place into iterators\n",
    "train_iterator, valid_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = 64,\n",
    "    sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Word2Vec (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_json('.data/train.json',orient='records',lines=True)\n",
    "\n",
    "w2v = Word2Vec(corpus.codes, size=300, workers=16, sg=1, min_count=3)\n",
    "w2v.save('.data/node_w2v_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04268723, -0.01990928, -0.10372822, ...,  0.34928635,\n",
       "        -0.24622028, -0.02363101],\n",
       "       [ 0.06655177, -0.08706249, -0.11346684, ...,  0.2967248 ,\n",
       "        -0.16500187, -0.10260527],\n",
       "       [ 0.10059763, -0.0993171 , -0.14234892, ...,  0.3913037 ,\n",
       "        -0.22237949,  0.02339004],\n",
       "       ...,\n",
       "       [-0.0031671 ,  0.01939397, -0.00094254, ..., -0.06062187,\n",
       "        -0.0873417 ,  0.10190531],\n",
       "       [-0.03719744,  0.02801778,  0.02174594, ..., -0.05577604,\n",
       "        -0.07265704, -0.00079473],\n",
       "       [ 0.01092949, -0.03061507, -0.045645  , ..., -0.06999503,\n",
       "        -0.15679213,  0.11291362]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = Word2Vec.load('.data/node_w2v_128')\n",
    "w2v.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embed): Embedding(8002, 128)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encode_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=MAX_VOCAB_SIZE+2):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer,self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=MAX_VOCAB_SIZE+2,\n",
    "                                  embedding_dim=128)\n",
    "        self.pos_encoder = PositionalEncoding(128, 0.1)\n",
    "        self.encode_layer = nn.TransformerEncoderLayer(d_model=128,\n",
    "                                                       nhead=8,\n",
    "                                                       dim_feedforward=2048,\n",
    "                                                       dropout=0.1,\n",
    "                                                       activation='relu')\n",
    "        self.trans_encoder = nn.TransformerEncoder(self.encode_layer,\n",
    "                                                   num_layers=1)\n",
    "        self.fc1 = nn.Linear(128,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.embed(x) * math.sqrt(128)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.trans_encoder(x)\n",
    "        x = self.fc1(x)\n",
    "        x = x[:,-1,:]\n",
    "        return(x)\n",
    "\n",
    "\n",
    "    \n",
    "model = Transformer()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word2Vec weights to embeddings (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.FloatTensor(w2v.wv.vectors)\n",
    "weights = weights.to(device)\n",
    "model.embed = model.embed.from_pretrained(weights)\n",
    "#model.embed = model.embed.weight.data.copy_(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,210,562 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_accuracy(probs,all_labels):\n",
    "    acc = (probs.argmax(1) == all_labels).sum()\n",
    "    acc = torch.div(acc,len(all_labels)+0.0)\n",
    "    return(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define optimizer\n",
    "#optimizer = SGD(model.parameters(), lr = 0.01)\n",
    "#optimizer = Adam(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "## Define loss function\n",
    "#criterion = nn.BCELoss().to(device) ## Sigmoid activation function\n",
    "#criterion = nn.NLLLoss().to(device) ### Log_softmax activation\n",
    "\n",
    "#weights = torch.tensor([1.0, 5.0])\n",
    "criterion = nn.CrossEntropyLoss(weight=weights).to(device) ## No activation function bcs softmax included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "for e in range(epochs):\n",
    "    running_acc = 0\n",
    "    running_loss = 0\n",
    "    timer = time.time()\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_iterator:\n",
    "        batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch.codes)\n",
    "        loss = criterion(output, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = softmax_accuracy(output,batch.label)\n",
    "        running_acc += acc.item()\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            running_acc_val = 0\n",
    "            running_loss_val = 0\n",
    "            for batch in valid_iterator:\n",
    "                batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "                output_val = model(batch.codes)\n",
    "                loss_val = criterion(output_val,batch.label)\n",
    "                acc_val = softmax_accuracy(output_val,batch.label)\n",
    "                running_acc_val += acc_val.item()\n",
    "                running_loss_val += loss_val.item()\n",
    "        \n",
    "        print(\"Epoch {} - Training acc: {:.6f} -Training loss: {:.6f} - Val acc: {:.6f} - Val loss: {:.6f} - Time: {:.4f}s\".format(e+1, running_acc/len(train_iterator), running_loss/len(train_iterator), running_acc_val/len(valid_iterator), running_loss_val/len(valid_iterator), (time.time()-timer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:  0.9376403443113772\n",
      "Train loss:  0.44436524781638276\n",
      "Confusion matrix: \n",
      " [[20000     0]\n",
      " [ 1333     0]]\n"
     ]
    }
   ],
   "source": [
    "### Evaluate on Training set\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    running_acc_test = 0\n",
    "    running_loss_test = 0\n",
    "    all_pred=[]\n",
    "    all_labels=[]\n",
    "    for batch in train_iterator:\n",
    "        batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "        output_test = model(batch.codes).squeeze(1)\n",
    "        loss_test = criterion(output_test,batch.label)\n",
    "        acc_test = softmax_accuracy(output_test,batch.label)\n",
    "        running_acc_test += acc_test.item()\n",
    "        running_loss_test += loss_test.item()\n",
    "        all_pred += output_test.tolist()\n",
    "        all_labels += batch.label.tolist()\n",
    "\n",
    "\n",
    "print('Train acc: ',running_acc_test/len(train_iterator))\n",
    "print('Train loss: ',running_loss_test/len(train_iterator))\n",
    "\n",
    "\n",
    "def getClass(x):\n",
    "    return(x.index(max(x)))\n",
    "\n",
    "probs = pd.Series(all_pred)\n",
    "all_predicted = probs.apply(getClass)\n",
    "all_predicted.reset_index(drop=True, inplace=True)\n",
    "vc = pd.value_counts(all_predicted == all_labels)\n",
    "\n",
    "confusion = sklearn.metrics.confusion_matrix(y_true=all_labels, y_pred=all_predicted)\n",
    "print('Confusion matrix: \\n',confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc:  0.937375992063492\n",
      "Test loss:  0.4417402297258377\n",
      "Confusion matrix: \n",
      " [[2494    0]\n",
      " [ 166    0]]\n",
      "\n",
      "TP: 0\n",
      "FP: 0\n",
      "TN: 2494\n",
      "FN: 166\n",
      "\n",
      "Accuracy: 0.937593984962406\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F-measure: 0.0\n",
      "Precision-Recall AUC: 0.06608821025076204\n",
      "AUC: 0.5040555163718226\n",
      "MCC: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/data/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "### Evaluate on Testing set\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    running_acc_test = 0\n",
    "    running_loss_test = 0\n",
    "    all_pred=[]\n",
    "    all_labels=[]\n",
    "    for batch in test_iterator:\n",
    "        batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "        output_test = model(batch.codes).squeeze(1)\n",
    "        loss_test = criterion(output_test,batch.label)\n",
    "        acc_test = softmax_accuracy(output_test,batch.label)\n",
    "        running_acc_test += acc_test.item()\n",
    "        running_loss_test += loss_test.item()\n",
    "        all_pred += output_test.tolist()\n",
    "        all_labels += batch.label.tolist()\n",
    "\n",
    "\n",
    "print('Test acc: ',running_acc_test/len(test_iterator))\n",
    "print('Test loss: ',running_loss_test/len(test_iterator))\n",
    "\n",
    "\n",
    "def getClass(x):\n",
    "    return(x.index(max(x)))\n",
    "\n",
    "probs = pd.Series(all_pred)\n",
    "all_predicted = probs.apply(getClass)\n",
    "all_predicted.reset_index(drop=True, inplace=True)\n",
    "vc = pd.value_counts(all_predicted == all_labels)\n",
    "\n",
    "probs2=[]\n",
    "for x in probs:\n",
    "    probs2.append(x[1])\n",
    "\n",
    "confusion = sklearn.metrics.confusion_matrix(y_true=all_labels, y_pred=all_predicted)\n",
    "print('Confusion matrix: \\n',confusion)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "print('\\nTP:',tp)\n",
    "print('FP:',fp)\n",
    "print('TN:',tn)\n",
    "print('FN:',fn)\n",
    "\n",
    "## Performance measure\n",
    "print('\\nAccuracy: '+ str(sklearn.metrics.accuracy_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "print('Precision: '+ str(sklearn.metrics.precision_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "print('Recall: '+ str(sklearn.metrics.recall_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "print('F-measure: '+ str(sklearn.metrics.f1_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "print('Precision-Recall AUC: '+ str(sklearn.metrics.average_precision_score(y_true=all_labels, y_score=probs2)))\n",
    "print('AUC: '+ str(sklearn.metrics.roc_auc_score(y_true=all_labels, y_score=probs2)))\n",
    "print('MCC: '+ str(sklearn.metrics.matthews_corrcoef(y_true=all_labels, y_pred=all_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS (Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights : 1:3 (400)\n",
    "val loss: 0.396747\n",
    "Test acc:  0.9320849867724867\n",
    "Test loss:  0.39700411295606974\n",
    "Confusion matrix: \n",
    " [[2471   23]\n",
    " [ 158    8]]\n",
    "\n",
    "TP: 8\n",
    "FP: 23\n",
    "TN: 2471\n",
    "FN: 158\n",
    "\n",
    "Accuracy: 0.9319548872180451\n",
    "Precision: 0.25806451612903225\n",
    "Recall: 0.04819277108433735\n",
    "F-measure: 0.08121827411167513\n",
    "Precision-Recall AUC: 0.17136589216762943\n",
    "AUC: 0.717964077641762\n",
    "MCC: 0.08783419878226471\n",
    "\n",
    "\n",
    "Weights: 1:15 (400)\n",
    "Test acc:  0.8045221560846562\n",
    "Test loss:  0.5900836005097344\n",
    "Confusion matrix: \n",
    " [[2050  444]\n",
    " [  76   90]]\n",
    "\n",
    "TP: 90\n",
    "FP: 444\n",
    "TN: 2050\n",
    "FN: 76\n",
    "\n",
    "Accuracy: 0.8045112781954887\n",
    "Precision: 0.16853932584269662\n",
    "Recall: 0.5421686746987951\n",
    "F-measure: 0.2571428571428571\n",
    "Precision-Recall AUC: 0.17323523591325418\n",
    "AUC: 0.7548574409909082\n",
    "MCC: 0.21989739199806438\n",
    "\n",
    "\n",
    "Weights: 1:5 (400)\n",
    "Test acc:  0.8738425925925926\n",
    "Test loss:  0.4650661984369868\n",
    "Confusion matrix: \n",
    " [[2252  242]\n",
    " [  94   72]]\n",
    "\n",
    "TP: 72\n",
    "FP: 242\n",
    "TN: 2252\n",
    "FN: 94\n",
    "\n",
    "Accuracy: 0.8736842105263158\n",
    "Precision: 0.22929936305732485\n",
    "Recall: 0.43373493975903615\n",
    "F-measure: 0.3\n",
    "Precision-Recall AUC: 0.1893410043394382\n",
    "AUC: 0.7772436981285205\n",
    "MCC: 0.2524173571221226\n",
    "\n",
    "\n",
    "Weights: 1:5 (400) - no weight decay- lr 0.0005\n",
    "Test acc:  0.908068783068783\n",
    "Test loss:  0.4351889540751775\n",
    "Confusion matrix: \n",
    " [[2352  142]\n",
    " [ 102   64]]\n",
    "\n",
    "TP: 64\n",
    "FP: 142\n",
    "TN: 2352\n",
    "FN: 102\n",
    "\n",
    "Accuracy: 0.9082706766917293\n",
    "Precision: 0.3106796116504854\n",
    "Recall: 0.3855421686746988\n",
    "F-measure: 0.3440860215053763\n",
    "Precision-Recall AUC: 0.2703683299643296\n",
    "AUC: 0.805646805344876\n",
    "MCC: 0.2973762185965168\n",
    "\n",
    "\n",
    "Weights: 1:5 (128)- no weight decay- lr 0.0005\n",
    "Test acc:  0.8882688492063492\n",
    "Test loss:  0.5016442422001135\n",
    "Confusion matrix: \n",
    " [[2271  223]\n",
    " [  75   91]]\n",
    "\n",
    "TP: 91\n",
    "FP: 223\n",
    "TN: 2271\n",
    "FN: 75\n",
    "\n",
    "Accuracy: 0.8879699248120301\n",
    "Precision: 0.2898089171974522\n",
    "Recall: 0.5481927710843374\n",
    "F-measure: 0.3791666666666667\n",
    "Precision-Recall AUC: 0.28130664351504053\n",
    "AUC: 0.8167771325880908\n",
    "MCC: 0.3439348556798436\n",
    "\n",
    "Weights: 1:5 (128) - lr 0.001\n",
    "Test acc:  0.8771908068783068\n",
    "Test loss:  0.4461185729929379\n",
    "Confusion matrix: \n",
    " [[2249  245]\n",
    " [  82   84]]\n",
    "\n",
    "TP: 84\n",
    "FP: 245\n",
    "TN: 2249\n",
    "FN: 82\n",
    "\n",
    "Accuracy: 0.8770676691729323\n",
    "Precision: 0.2553191489361702\n",
    "Recall: 0.5060240963855421\n",
    "F-measure: 0.3393939393939393\n",
    "Precision-Recall AUC: 0.2500610212460307\n",
    "AUC: 0.8273470787721858\n",
    "MCC: 0.2996180523004313\n",
    "\n",
    "CLANG\n",
    "\n",
    "Epoch 1 - Training acc: 0.922811 -Training loss: 0.498573 - Val acc: 0.893601 - Val loss: 0.479064 - Time: 172.8738s\n",
    "Epoch 2 - Training acc: 0.893287 -Training loss: 0.445196 - Val acc: 0.924851 - Val loss: 0.494235 - Time: 172.5547s\n",
    "Epoch 3 - Training acc: 0.893058 -Training loss: 0.422515 - Val acc: 0.854497 - Val loss: 0.470852 - Time: 176.5896s\n",
    "Epoch 4 - Training acc: 0.900021 -Training loss: 0.391255 - Val acc: 0.882027 - Val loss: 0.442497 - Time: 186.8855s\n",
    "Epoch 5 - Training acc: 0.903768 -Training loss: 0.367009 - Val acc: 0.898686 - Val loss: 0.449274 - Time: 182.4509s\n",
    "Epoch 6 - Training acc: 0.908117 -Training loss: 0.337508 - Val acc: 0.845982 - Val loss: 0.459853 - Time: 184.6313s\n",
    "Epoch 7 - Training acc: 0.911911 -Training loss: 0.308918 - Val acc: 0.885003 - Val loss: 0.440262 - Time: 181.9972s\n",
    "Epoch 8 - Training acc: 0.917375 -Training loss: 0.290550 - Val acc: 0.885003 - Val loss: 0.449175 - Time: 182.8071s\n",
    "Epoch 9 - Training acc: 0.919396 -Training loss: 0.274571 - Val acc: 0.877935 - Val loss: 0.486241 - Time: 185.6923s\n",
    "Epoch 10 - Training acc: 0.924305 -Training loss: 0.255520 - Val acc: 0.891493 - Val loss: 0.547270 - Time: 185.0608s\n",
    "Epoch 11 - Training acc: 0.927576 -Training loss: 0.235803 - Val acc: 0.890129 - Val loss: 0.485028 - Time: 184.5970s\n",
    "Epoch 12 - Training acc: 0.935208 -Training loss: 0.218208 - Val acc: 0.855861 - Val loss: 0.594641 - Time: 184.4211s\n",
    "\n",
    "\n",
    "Mytokern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
