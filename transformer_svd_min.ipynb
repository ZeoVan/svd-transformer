{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground for Transformers!\n",
    "\n",
    "### Attention is all you need \n",
    "(https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "### For software vulnerability detection GYM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a minimal example of this **CRAZY** idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LSTM is working with:\n",
    "    * Adam Learning rate = 0.01\n",
    "    * Overfitted on 14 epoch. 100% training accuracy.\n",
    "    * Worked on 1K dataset sample\n",
    "    * Using the `hidden`n or `cell` output from LSTM. Not the `output`.\n",
    "    * Bidrectional (2 layers)\n",
    "* Transformer:\n",
    "    * Still trying to find the right combination\n",
    "    * Adam Learning rate 0.0001\n",
    "    * Beat this:\n",
    "     Epoch 46 - Training acc: 0.824250 -Training loss: 0.426513 - Val acc: 0.775586 - Val loss: 0.496733 - Time: 102.7327s\n",
    "     \n",
    "* 100K run:  \n",
    "Epoch 1 - Training acc: 0.674675 -Training loss: 0.601032 - Val acc: 0.700293 - Val loss: 0.565769 - Time: 1541.4087s\n",
    "Epoch 2 - Training acc: 0.739337 -Training loss: 0.538962 - Val acc: 0.741205 - Val loss: 0.523540 - Time: 1499.5019s\n",
    "Epoch 3 - Training acc: 0.759912 -Training loss: 0.512259 - Val acc: 0.701826 - Val loss: 0.561835 - Time: 1408.9301s\n",
    "Epoch 4 - Training acc: 0.762787 -Training loss: 0.509124 - Val acc: 0.758418 - Val loss: 0.498503 - Time: 1412.1519s\n",
    "Epoch 5 - Training acc: 0.766650 -Training loss: 0.505486 - Val acc: 0.708733 - Val loss: 0.551428 - Time: 1412.9424s\n",
    "Epoch 6 - Training acc: 0.771212 -Training loss: 0.499414 - Val acc: 0.755797 - Val loss: 0.500520 - Time: 1406.9010s\n",
    "Epoch 7 - Training acc: 0.773650 -Training loss: 0.496225 - Val acc: 0.766147 - Val loss: 0.481232 - Time: 1409.5581s\n",
    "Epoch 8 - Training acc: 0.776613 -Training loss: 0.493755 - Val acc: 0.725013 - Val loss: 0.553314 - Time: 1419.0735s\n",
    "Epoch 9 - Training acc: 0.780300 -Training loss: 0.486875 - Val acc: 0.742471 - Val loss: 0.510461 - Time: 1411.7488s\n",
    "Epoch 10 - Training acc: 0.785575 -Training loss: 0.480348 - Val acc: 0.782205 - Val loss: 0.456913 - Time: 1414.1186s\n",
    "Epoch 11 - Training acc: 0.788850 -Training loss: 0.472767 - Val acc: 0.792089 - Val loss: 0.445218 - Time: 1424.2309s\n",
    "Epoch 12 - Training acc: 0.797475 -Training loss: 0.459784 - Val acc: 0.780095 - Val loss: 0.466281 - Time: 1407.5756s\n",
    "Epoch 13 - Training acc: 0.804100 -Training loss: 0.448606 - Val acc: 0.780162 - Val loss: 0.482026 - Time: 1438.1905s\n",
    "Epoch 14 - Training acc: 0.809700 -Training loss: 0.442654 - Val acc: 0.795020 - Val loss: 0.451707 - Time: 1439.5672s\n",
    "Epoch 15 - Training acc: 0.812675 -Training loss: 0.438664 - Val acc: 0.802461 - Val loss: 0.448924 - Time: 1435.7601s\n",
    "Epoch 16 - Training acc: 0.819150 -Training loss: 0.428220 - Val acc: 0.793888 - Val loss: 0.473866 - Time: 1419.7033s\n",
    "Epoch 17 - Training acc: 0.821538 -Training loss: 0.423442 - Val acc: 0.812744 - Val loss: 0.425167 - Time: 1409.6141s\n",
    "Epoch 18 - Training acc: 0.825037 -Training loss: 0.417003 - Val acc: 0.806414 - Val loss: 0.443101 - Time: 1411.5896s\n",
    "Epoch 19 - Training acc: 0.827325 -Training loss: 0.411255 - Val acc: 0.790378 - Val loss: 0.482602 - Time: 1413.5015s\n",
    "Epoch 20 - Training acc: 0.827762 -Training loss: 0.411628 - Val acc: 0.793643 - Val loss: 0.474180 - Time: 1418.7839s\n",
    "Epoch 21 - Training acc: 0.831237 -Training loss: 0.406693 - Val acc: 0.811878 - Val loss: 0.430180 - Time: 1412.6029s\n",
    "Epoch 22 - Training acc: 0.832862 -Training loss: 0.405046 - Val acc: 0.796842 - Val loss: 0.491119 - Time: 1411.8020s\n",
    "Epoch 23 - Training acc: 0.837500 -Training loss: 0.394457 - Val acc: 0.810079 - Val loss: 0.436688 - Time: 1410.8785s\n",
    "Epoch 24 - Training acc: 0.840300 -Training loss: 0.390056 - Val acc: 0.812278 - Val loss: 0.440547 - Time: 1434.1152s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torchtext.vocab as vocab\n",
    "import sklearn.metrics\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.optim import SGD,Adam\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "cudnn.benchmark = True\n",
    "cudnn.enabled = True\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load playset dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle('playset(0.25.2).pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>functionSource</th>\n",
       "      <th>combine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93792</th>\n",
       "      <td>go_file_opener_open (GOFileOpener const *fo, g...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79770</th>\n",
       "      <td>updatePathMap(bool left_level) {\\n\\tPoint from...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66999</th>\n",
       "      <td>interpret_tilde(const char* path) {\\n    stati...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44284</th>\n",
       "      <td>checkVarExp(\\n        Absyn *node,\\n        Ta...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49515</th>\n",
       "      <td>will_have_skip_worktree(const struct cache_ent...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96701</th>\n",
       "      <td>AVLTree_insert(AVLTree * tree, void * data)\\n{...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67815</th>\n",
       "      <td>remove_hook(const char *name, hookfn fn)\\n{\\n\\...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88363</th>\n",
       "      <td>output_def(dico_stream_t str, struct gcide_db ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65929</th>\n",
       "      <td>getState(\\n\\t\\tFLMUINT\\t\\tuiFieldID)\\n\\t{\\n\\t\\...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16426</th>\n",
       "      <td>untag_proplist(Pulse_Tag *tag, Eina_Hash **pro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          functionSource  combine\n",
       "93792  go_file_opener_open (GOFileOpener const *fo, g...    False\n",
       "79770  updatePathMap(bool left_level) {\\n\\tPoint from...    False\n",
       "66999  interpret_tilde(const char* path) {\\n    stati...    False\n",
       "44284  checkVarExp(\\n        Absyn *node,\\n        Ta...     True\n",
       "49515  will_have_skip_worktree(const struct cache_ent...     True\n",
       "...                                                  ...      ...\n",
       "96701  AVLTree_insert(AVLTree * tree, void * data)\\n{...    False\n",
       "67815  remove_hook(const char *name, hookfn fn)\\n{\\n\\...    False\n",
       "88363  output_def(dico_stream_t str, struct gcide_db ...    False\n",
       "65929  getState(\\n\\t\\tFLMUINT\\t\\tuiFieldID)\\n\\t{\\n\\t\\...    False\n",
       "16426  untag_proplist(Pulse_Tag *tag, Eina_Hash **pro...     True\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>functionSource</th>\n",
       "      <th>combine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>100000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>GetName(char *str,int strLen) const\\n{\\n    if...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           functionSource combine\n",
       "count                                              100000  100000\n",
       "unique                                             100000       2\n",
       "top     GetName(char *str,int strLen) const\\n{\\n    if...    True\n",
       "freq                                                    1   50000"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file_replace(struct magic_set *ms, const char *pat, const char *rep)\\n{\\n\\tregex_t rx;\\n\\tint rc;\\n\\n\\trc = regcomp(&rx, pat, REG_EXTENDED);\\n\\tif (rc) {\\n\\t\\tchar errmsg[512];\\n\\t\\t(void)regerror(rc, &rx, errmsg, sizeof(errmsg));\\n\\t\\tfile_magerror(ms, \"regex error %d, (%s)\", rc, errmsg);\\n\\t\\treturn -1;\\n\\t} else {\\n\\t\\tregmatch_t rm;\\n\\t\\tint nm = 0;\\n\\t\\twhile (regexec(&rx, ms->o.buf, 1, &rm, 0) == 0) {\\n\\t\\t\\tms->o.buf[rm.rm_so] = \\'\\\\0\\';\\n\\t\\t\\tif (file_printf(ms, \"%s%s\", rep,\\n\\t\\t\\t    rm.rm_eo != 0 ? ms->o.buf + rm.rm_eo : \"\") == -1)\\n\\t\\t\\t\\treturn -1;\\n\\t\\t\\tnm++;\\n\\t\\t}\\n\\t\\tregfree(&rx);\\n\\t\\treturn nm;\\n\\t}\\n}'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.functionSource[24492]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data (in JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaner & Tokkenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "global c\n",
    "c=0\n",
    "def my_tokenizer(code):\n",
    "    global c\n",
    "    c+=1\n",
    "    print('{0}'.format(c), end='\\r')\n",
    "    \n",
    "    pat = re.compile(r'(/\\*([^*]|(\\*+[^*/]))*\\*+/)|(//.*)')\n",
    "    code = re.sub(pat,'',code)\n",
    "    \n",
    "    ## Remove code comments\n",
    "    #code = re.sub(r'(/\\*([^*]|[\\r\\n]|(\\*+([^*/]|[\\r\\n])))*\\*+/)|(//.*)','',code)\n",
    "    \n",
    "    ## Remove newlines & tabs\n",
    "    code = re.sub('(\\n)|(\\\\\\\\n)|(\\\\\\\\)|(\\\\t)|(/)|(\\\\r)','',code)\n",
    "    \n",
    "    ## Mix split (characters and words)\n",
    "    splitter = '\\\"(.*?)\\\"| +|(;)|(\\()|(==)|(\\))|(=)|(\\+)|(\\-)|(\\[)|(\\])|(<)|(>)|(\\.)|({)'\n",
    "    code = re.split(splitter,code)\n",
    "    \n",
    "    ## Remove None type\n",
    "    code = list(filter(None, code))\n",
    "    \n",
    "    code = list(filter(str.strip, code))\n",
    "    #code = \" \".join(code)\n",
    "    \n",
    "    ## Return list of tokens\n",
    "    return(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100002\r"
     ]
    }
   ],
   "source": [
    "dataset = dataset.functionSource.apply(my_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change labels boolean to 1 and 0\n",
    "dataset.iloc[:,1] = np.multiply(dataset.iloc[:,1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change column name\n",
    "dataset = dataset.rename(columns={'functionSource':'codes', 'combine':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "false = dataset[dataset.iloc[:,1]==0]\n",
    "true = dataset[dataset.iloc[:,1]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split to train,test,valid\n",
    "train = false[0:20000].append(true[0:1333])\n",
    "test  = false[20000:22494].append(true[2000:2166])\n",
    "valid = false[30000:32494].append(true[3000:3166])\n",
    "\n",
    "## Shuffle\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "test = test.sample(frac=1).reset_index(drop=True)\n",
    "valid = valid.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to json\n",
    "train.to_json('.data/train_ratio.json', orient='records',lines=True)\n",
    "test.to_json('.data/test_ratio.json', orient='records',lines=True)\n",
    "valid.to_json('.data/valid_ratio.json', orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the field\n",
    "\n",
    "CODES = torchtext.data.Field(batch_first=True, tokenize=my_tokenizer)\n",
    "LABEL = torchtext.data.LabelField(dtype=torch.long)\n",
    "fields = {'codes': ('codes', CODES), 'label': ('label', LABEL)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import ratio dataset 20K\n",
    "train_data, valid_data, test_data = torchtext.data.TabularDataset.splits(\n",
    "                                        path = '.data',\n",
    "                                        train = 'train_ratio.json',\n",
    "                                        validation = 'valid_ratio.json',\n",
    "                                        test = 'test_ratio.json',\n",
    "                                        format = 'json',\n",
    "                                        fields = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the 1K data as TabularDataset\n",
    "train_data, valid_data, test_data = torchtext.data.TabularDataset.splits(\n",
    "                                        path = '.data',\n",
    "                                        train = 'train_1k.json',\n",
    "                                        validation = 'valid_1k.json',\n",
    "                                        test = 'test_1k.json',\n",
    "                                        format = 'json',\n",
    "                                        fields = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the min(10k) data as TabularDataset\n",
    "train_data, valid_data, test_data = torchtext.data.TabularDataset.splits(\n",
    "                                        path = '.data',\n",
    "                                        train = 'train_min.json',\n",
    "                                        validation = 'valid_min.json',\n",
    "                                        test = 'test_min.json',\n",
    "                                        format = 'json',\n",
    "                                        fields = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the 100K data as TabularDataset\n",
    "train_data, valid_data, test_data = torchtext.data.TabularDataset.splits(\n",
    "                                        path = '.data',\n",
    "                                        train = 'train_100K.json',\n",
    "                                        validation = 'valid_100k.json',\n",
    "                                        test = 'test_100k.json',\n",
    "                                        format = 'json',\n",
    "                                        fields = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'codes': ['ap2sta_data_frame', '(', 'struct', '_adapter', '*adapter,', 'union', 'recv_frame', '*precv_frame,', 'struct', 'sta_info', '**psta', ')', '{', 'u8', '*ptr', '=', 'precv_frame', '-', '>', 'u.hdr.rx_data', ';', 'struct', 'rx_pkt_attrib', '*pattrib', '=', '&precv_frame', '-', '>', 'u.hdr.attrib', ';', 'structsta_priv', '*pstapriv', '=', '&adapter', '-', '>', 'stapriv', ';', 'structmlme_priv', '*pmlmepriv', '=', '&adapter', '-', '>', 'mlmepriv', ';', 'u8', '*mybssid', '=', 'get_bssid', '(', 'pmlmepriv', ')', ';', 'u8', '*myhwaddr', '=', 'myid', '(', '&adapter', '-', '>', 'eeprompriv', ')', ';', 'sint', 'bmcast', '=', 'IS_MCAST', '(', 'pattrib', '-', '>', 'dst', ')', ';', 'if', '(', 'check_fwstate', '(', 'pmlmepriv,', 'WIFI_STATION_STATE', ')', '&&', 'check_fwstate', '(', 'pmlmepriv,', '_FW_LINKED', ')', ')', '{', '*', 'if', 'NULL', '-', 'frame,', 'drop', 'packet', '*if', '(', '(', 'GetFrameSubType', '(', 'ptr', ')', ')', '==', 'WIFI_DATA_NULL', ')', 'return', '_FAIL', ';', '*', 'drop', 'QoS', '-', 'SubType', 'Data,', 'including', 'QoS', 'NULL,', '*', 'excluding', 'QoS', '-', 'Data', '*if', '(', '(', 'GetFrameSubType', '(', 'ptr', ')', '&', 'WIFI_QOS_DATA_TYPE', ')', '==', 'WIFI_QOS_DATA_TYPE', ')', '{', 'if', '(', 'GetFrameSubType', '(', 'ptr', ')', '&', '(', 'BIT', '(', '4', ')', '|', 'BIT', '(', '5', ')', '|', 'BIT', '(', '6', ')', ')', ')', 'return', '_FAIL', ';', '}*', 'filter', 'packets', 'that', 'SA', 'is', 'myself', 'or', 'multicast', 'or', 'broadcast', '*', 'if', '(', '!memcmp', '(', 'myhwaddr,', 'pattrib', '-', '>', 'src,', 'ETH_ALEN', ')', ')', 'return', '_FAIL', ';', '*', 'da', 'should', 'be', 'for', 'me', '*if', '(', '(', 'memcmp', '(', 'myhwaddr,', 'pattrib', '-', '>', 'dst,', 'ETH_ALEN', ')', ')', '&&', '(', '!bmcast', ')', ')', 'return', '_FAIL', ';', '*', 'check', 'BSSID', '*if', '(', 'is_zero_ether_addr', '(', 'pattrib', '-', '>', 'bssid', ')', '||', 'is_zero_ether_addr', '(', 'mybssid', ')', '||', '(', 'memcmp', '(', 'pattrib', '-', '>', 'bssid,', 'mybssid,', 'ETH_ALEN', ')', ')', ')', 'return', '_FAIL', ';', 'if', '(', 'bmcast', ')', '*psta', '=', 'r8712_get_bcmc_stainfo', '(', 'adapter', ')', ';', 'else', '*psta', '=', 'r8712_get_stainfo', '(', 'pstapriv,', 'pattrib', '-', '>', 'bssid', ')', ';', 'if', '(', '*psta', '==', 'NULL', ')', 'return', '_FAIL', ';', '}', 'else', 'if', '(', 'check_fwstate', '(', 'pmlmepriv,', 'WIFI_MP_STATE', ')', '&&', 'check_fwstate', '(', 'pmlmepriv,', '_FW_LINKED', ')', ')', '{', 'memcpy', '(', 'pattrib', '-', '>', 'dst,', 'GetAddr1Ptr', '(', 'ptr', ')', ',', 'ETH_ALEN', ')', ';', 'memcpy', '(', 'pattrib', '-', '>', 'src,', 'GetAddr2Ptr', '(', 'ptr', ')', ',', 'ETH_ALEN', ')', ';', 'memcpy', '(', 'pattrib', '-', '>', 'bssid,', 'GetAddr3Ptr', '(', 'ptr', ')', ',', 'ETH_ALEN', ')', ';', 'memcpy', '(', 'pattrib', '-', '>', 'ra,', 'pattrib', '-', '>', 'dst,', 'ETH_ALEN', ')', ';', 'memcpy', '(', 'pattrib', '-', '>', 'ta,', 'pattrib', '-', '>', 'src,', 'ETH_ALEN', ')', ';', 'memcpy', '(', 'pattrib', '-', '>', 'bssid,', 'mybssid,', 'ETH_ALEN', ')', ';', '*psta', '=', 'r8712_get_stainfo', '(', 'pstapriv,', 'pattrib', '-', '>', 'bssid', ')', ';', 'if', '(', '*psta', '==', 'NULL', ')', 'return', '_FAIL', ';', '}', 'else', '{', 'return', '_FAIL', ';', '}return', '_SUCCESS', ';', '}'], 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "print(vars(valid_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doneeeeeeeeeeeeeeeee !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary-related preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Build the vocabulary\n",
    "\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "\n",
    "CODES.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 10002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(CODES.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(')', 1376013), ('(', 1375892), (';', 1270722), ('-', 656492), ('=', 633211), ('>', 507801), ('{', 307771), ('*', 260258), ('if', 254846), ('0', 200507), ('}', 197609), ('+', 187616), ('[', 166766), (']', 166621), ('return', 131170), ('i', 118283), ('1', 114002), ('==', 106852), ('<', 98924), ('NULL', 87699), ('int', 86141), ('char', 72021), ('the', 69537), ('!', 61198), ('struct', 55464), ('for', 48509), ('else', 45145), ('to', 41012), ('const', 40896), ('&&', 38090), ('#', 32367), ('case', 32198), ('sizeof', 32070), ('break', 30391), ('%s', 30237), ('a', 28443), ('buf', 27300), ('p', 26983), ('len', 25757), ('ret', 25649), ('||', 24762), ('data', 24691), ('2', 24650), ('&', 24588), ('unsigned', 24438), ('is', 23138), ('name', 21712), ('of', 21539), ('this', 20787), ('in', 20509), ('s', 19776), ('n', 19228), ('j', 18797), (':', 18123), ('c', 17438), ('size', 17372), ('file', 16497), ('void', 16480), ('while', 16332), ('%d', 15705), ('we', 14985), ('and', 14598), ('type', 14553), ('not', 14500), ('goto', 14244), ('x', 14124), ('}}', 14000), ('error', 13602), ('|', 13518), ('buffer', 12624), ('r', 12619), ('result', 12525), ('value', 12430), ('?', 12325), ('*if', 11847), ('fd', 11582), ('fprintf', 11544), ('3', 11370), ('line', 11117), ('it', 11007), ('4', 10960), ('next', 10878), ('}if', 10749), ('out', 10355), ('t', 10255), ('err', 10041), ('size_t', 10038), ('strlen', 10038), ('be', 10004), ('long', 9794), ('count', 9745), ('FALSE', 9704), ('free', 9579), ('memcpy', 9319), ('f', 9176), ('flags', 9013), ('priv', 8909), ('false', 8909), ('path', 8854), ('length', 8589)]\n"
     ]
    }
   ],
   "source": [
    "## Most common word\n",
    "print(CODES.vocab.freqs.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', ')', '(', ';', '-', '=', '>', '{', '*']\n",
      "defaultdict(None, {0: 0, 1: 1})\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(CODES.vocab.itos[:10])\n",
    "print(LABEL.vocab.stoi)\n",
    "print(CODES.vocab.stoi[CODES.pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## place into iterators\n",
    "train_iterator, valid_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = 32,\n",
    "    sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Word2Vec (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_json('.data/train.json',orient='records',lines=True)\n",
    "\n",
    "w2v = Word2Vec(corpus.codes, size=300, workers=16, sg=1, min_count=3)\n",
    "w2v.save('.data/node_w2v_128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04268723, -0.01990928, -0.10372822, ...,  0.34928635,\n",
       "        -0.24622028, -0.02363101],\n",
       "       [ 0.06655177, -0.08706249, -0.11346684, ...,  0.2967248 ,\n",
       "        -0.16500187, -0.10260527],\n",
       "       [ 0.10059763, -0.0993171 , -0.14234892, ...,  0.3913037 ,\n",
       "        -0.22237949,  0.02339004],\n",
       "       ...,\n",
       "       [-0.0031671 ,  0.01939397, -0.00094254, ..., -0.06062187,\n",
       "        -0.0873417 ,  0.10190531],\n",
       "       [-0.03719744,  0.02801778,  0.02174594, ..., -0.05577604,\n",
       "        -0.07265704, -0.00079473],\n",
       "       [ 0.01092949, -0.03061507, -0.045645  , ..., -0.06999503,\n",
       "        -0.15679213,  0.11291362]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = Word2Vec.load('.data/node_w2v_128')\n",
    "w2v.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer class (with LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embed): Embedding(10002, 400)\n",
      "  (encode_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): Linear(in_features=400, out_features=400, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=400, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=400, bias=True)\n",
      "    (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (trans_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=400, out_features=400, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=400, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=400, bias=True)\n",
      "        (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lstm1): LSTM(400, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer,self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=10002,\n",
    "                                  embedding_dim=400)\n",
    "        self.encode_layer = nn.TransformerEncoderLayer(d_model=400,\n",
    "                                                       nhead=8,\n",
    "                                                       dim_feedforward=2048,\n",
    "                                                       dropout=0.1,\n",
    "                                                       activation='relu')\n",
    "        self.trans_encoder = nn.TransformerEncoder(self.encode_layer,\n",
    "                                                   num_layers=1)\n",
    "        self.lstm1 = nn.LSTM(input_size=400,\n",
    "                            hidden_size=128,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(128*2,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.embed(x)\n",
    "        x = self.trans_encoder(x)\n",
    "        output, (hidden, cell) = self.lstm1(x)\n",
    "        x = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        x = self.fc1(x)\n",
    "        return(x)\n",
    "\n",
    "model = Transformer()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word2Vec weights to embeddings (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.FloatTensor(w2v.wv.vectors)\n",
    "weights = weights.to(device)\n",
    "model.embed = model.embed.from_pretrained(weights)\n",
    "#model.embed = model.embed.weight.data.copy_(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9,507,394 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_accuracy(probs,all_labels):\n",
    "    def getClass(x):\n",
    "        return(x.index(max(x)))\n",
    "    \n",
    "    all_labels = all_labels.tolist()\n",
    "    probs = pd.Series(probs.tolist())\n",
    "    all_predicted = probs.apply(getClass)\n",
    "    all_predicted.reset_index(drop=True, inplace=True)\n",
    "    vc = pd.value_counts(all_predicted == all_labels)\n",
    "    try:\n",
    "        acc = vc[1]/len(all_labels)\n",
    "    except:\n",
    "        if(vc.index[0]==False):\n",
    "            acc = 0\n",
    "        else:\n",
    "            acc = 1\n",
    "    return(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define optimizer\n",
    "#optimizer = SGD(model.parameters(), lr = 0.01)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "\n",
    "## Define loss function\n",
    "#criterion = nn.BCELoss().to(device) ## Sigmoid activation function\n",
    "#criterion = nn.NLLLoss().to(device) ### Log_softmax activation\n",
    "criterion = nn.CrossEntropyLoss().to(device) ## No activation function bcs softmax included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training acc: 0.674675 -Training loss: 0.601032 - Val acc: 0.700293 - Val loss: 0.565769 - Time: 1541.4087s\n",
      "Epoch 2 - Training acc: 0.739337 -Training loss: 0.538962 - Val acc: 0.741205 - Val loss: 0.523540 - Time: 1499.5019s\n",
      "Epoch 3 - Training acc: 0.759912 -Training loss: 0.512259 - Val acc: 0.701826 - Val loss: 0.561835 - Time: 1408.9301s\n",
      "Epoch 4 - Training acc: 0.762787 -Training loss: 0.509124 - Val acc: 0.758418 - Val loss: 0.498503 - Time: 1412.1519s\n",
      "Epoch 5 - Training acc: 0.766650 -Training loss: 0.505486 - Val acc: 0.708733 - Val loss: 0.551428 - Time: 1412.9424s\n",
      "Epoch 6 - Training acc: 0.771212 -Training loss: 0.499414 - Val acc: 0.755797 - Val loss: 0.500520 - Time: 1406.9010s\n",
      "Epoch 7 - Training acc: 0.773650 -Training loss: 0.496225 - Val acc: 0.766147 - Val loss: 0.481232 - Time: 1409.5581s\n",
      "Epoch 8 - Training acc: 0.776613 -Training loss: 0.493755 - Val acc: 0.725013 - Val loss: 0.553314 - Time: 1419.0735s\n",
      "Epoch 9 - Training acc: 0.780300 -Training loss: 0.486875 - Val acc: 0.742471 - Val loss: 0.510461 - Time: 1411.7488s\n",
      "Epoch 10 - Training acc: 0.785575 -Training loss: 0.480348 - Val acc: 0.782205 - Val loss: 0.456913 - Time: 1414.1186s\n",
      "Epoch 11 - Training acc: 0.788850 -Training loss: 0.472767 - Val acc: 0.792089 - Val loss: 0.445218 - Time: 1424.2309s\n",
      "Epoch 12 - Training acc: 0.797475 -Training loss: 0.459784 - Val acc: 0.780095 - Val loss: 0.466281 - Time: 1407.5756s\n",
      "Epoch 13 - Training acc: 0.804100 -Training loss: 0.448606 - Val acc: 0.780162 - Val loss: 0.482026 - Time: 1438.1905s\n",
      "Epoch 14 - Training acc: 0.809700 -Training loss: 0.442654 - Val acc: 0.795020 - Val loss: 0.451707 - Time: 1439.5672s\n",
      "Epoch 15 - Training acc: 0.812675 -Training loss: 0.438664 - Val acc: 0.802461 - Val loss: 0.448924 - Time: 1435.7601s\n",
      "Epoch 16 - Training acc: 0.819150 -Training loss: 0.428220 - Val acc: 0.793888 - Val loss: 0.473866 - Time: 1419.7033s\n",
      "Epoch 17 - Training acc: 0.821538 -Training loss: 0.423442 - Val acc: 0.812744 - Val loss: 0.425167 - Time: 1409.6141s\n",
      "Epoch 18 - Training acc: 0.825037 -Training loss: 0.417003 - Val acc: 0.806414 - Val loss: 0.443101 - Time: 1411.5896s\n",
      "Epoch 19 - Training acc: 0.827325 -Training loss: 0.411255 - Val acc: 0.790378 - Val loss: 0.482602 - Time: 1413.5015s\n",
      "Epoch 20 - Training acc: 0.827762 -Training loss: 0.411628 - Val acc: 0.793643 - Val loss: 0.474180 - Time: 1418.7839s\n",
      "Epoch 21 - Training acc: 0.831237 -Training loss: 0.406693 - Val acc: 0.811878 - Val loss: 0.430180 - Time: 1412.6029s\n",
      "Epoch 22 - Training acc: 0.832862 -Training loss: 0.405046 - Val acc: 0.796842 - Val loss: 0.491119 - Time: 1411.8020s\n",
      "Epoch 23 - Training acc: 0.837500 -Training loss: 0.394457 - Val acc: 0.810079 - Val loss: 0.436688 - Time: 1410.8785s\n",
      "Epoch 24 - Training acc: 0.840300 -Training loss: 0.390056 - Val acc: 0.812278 - Val loss: 0.440547 - Time: 1434.1152s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b774a30ec04e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0moutput_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m                 \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0macc_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hazim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-0526da5d831b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrans_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hazim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hazim\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 559\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    560\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=100\n",
    "for e in range(epochs):\n",
    "    running_acc = 0\n",
    "    running_loss = 0\n",
    "    timer = time.time()\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_iterator:\n",
    "        batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch.codes)\n",
    "        loss = criterion(output, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = softmax_accuracy(output,batch.label)\n",
    "        running_acc += acc.item()\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            running_acc_val = 0\n",
    "            running_loss_val = 0\n",
    "            for batch in valid_iterator:\n",
    "                batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "                output_val = model(batch.codes)\n",
    "                loss_val = criterion(output_val,batch.label)\n",
    "                acc_val = softmax_accuracy(output_val,batch.label)\n",
    "                running_acc_val += acc_val.item()\n",
    "                running_loss_val += loss_val.item()\n",
    "        \n",
    "        print(\"Epoch {} - Training acc: {:.6f} -Training loss: {:.6f} - Val acc: {:.6f} - Val loss: {:.6f} - Time: {:.4f}s\".format(e+1, running_acc/len(train_iterator), running_loss/len(train_iterator), running_acc_val/len(valid_iterator), running_loss_val/len(valid_iterator), (time.time()-timer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:  0.852\n",
      "Train loss:  0.3846263731718063\n",
      "Confusion matrix: \n",
      " [[3562  438]\n",
      " [ 746 3254]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    running_acc_test = 0\n",
    "    running_loss_test = 0\n",
    "    all_pred=[]\n",
    "    all_labels=[]\n",
    "    for batch in train_iterator:\n",
    "        batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "        output_test = model(batch.codes).squeeze(1)\n",
    "        loss_test = criterion(output_test,batch.label)\n",
    "        acc_test = softmax_accuracy(output_test,batch.label)\n",
    "        running_acc_test += acc_test.item()\n",
    "        running_loss_test += loss_test.item()\n",
    "        all_pred += output_test.tolist()\n",
    "        all_labels += batch.label.tolist()\n",
    "\n",
    "\n",
    "print('Train acc: ',running_acc_test/len(train_iterator))\n",
    "print('Train loss: ',running_loss_test/len(train_iterator))\n",
    "\n",
    "\n",
    "def getClass(x):\n",
    "    return(x.index(max(x)))\n",
    "\n",
    "probs = pd.Series(all_pred)\n",
    "all_predicted = probs.apply(getClass)\n",
    "all_predicted.reset_index(drop=True, inplace=True)\n",
    "vc = pd.value_counts(all_predicted == all_labels)\n",
    "\n",
    "confusion = sklearn.metrics.confusion_matrix(y_true=all_labels, y_pred=all_predicted)\n",
    "print('Confusion matrix: \\n',confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc:  0.8237819488817891\n",
      "Test loss:  0.434028828153595\n",
      "Confusion matrix: \n",
      " [[4235  765]\n",
      " [ 999 4001]]\n",
      "\n",
      "TP: 4001\n",
      "FP: 765\n",
      "TN: 4235\n",
      "FN: 999\n",
      "\n",
      "Accuracy: 0.8236\n",
      "Precision: 0.8394880402853546\n",
      "Recall: 0.8002\n",
      "F-measure: 0.8193733360638952\n",
      "Precision-Recall AUC: 0.8626872364818003\n",
      "AUC: 0.8773458799999999\n",
      "MCC: 0.6479099280617139\n"
     ]
    }
   ],
   "source": [
    "### SOFTMAX\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    running_acc_test = 0\n",
    "    running_loss_test = 0\n",
    "    all_pred=[]\n",
    "    all_labels=[]\n",
    "    for batch in test_iterator:\n",
    "        batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "        output_test = model(batch.codes).squeeze(1)\n",
    "        loss_test = criterion(output_test,batch.label)\n",
    "        acc_test = softmax_accuracy(output_test,batch.label)\n",
    "        running_acc_test += acc_test.item()\n",
    "        running_loss_test += loss_test.item()\n",
    "        all_pred += output_test.tolist()\n",
    "        all_labels += batch.label.tolist()\n",
    "\n",
    "\n",
    "print('Test acc: ',running_acc_test/len(test_iterator))\n",
    "print('Test loss: ',running_loss_test/len(test_iterator))\n",
    "\n",
    "\n",
    "def getClass(x):\n",
    "    return(x.index(max(x)))\n",
    "\n",
    "probs = pd.Series(all_pred)\n",
    "all_predicted = probs.apply(getClass)\n",
    "all_predicted.reset_index(drop=True, inplace=True)\n",
    "vc = pd.value_counts(all_predicted == all_labels)\n",
    "\n",
    "probs2=[]\n",
    "for x in probs:\n",
    "    probs2.append(x[1])\n",
    "\n",
    "confusion = sklearn.metrics.confusion_matrix(y_true=all_labels, y_pred=all_predicted)\n",
    "print('Confusion matrix: \\n',confusion)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "print('\\nTP:',tp)\n",
    "print('FP:',fp)\n",
    "print('TN:',tn)\n",
    "print('FN:',fn)\n",
    "\n",
    "## Performance measure\n",
    "print('\\nAccuracy: '+ str(sklearn.metrics.accuracy_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "print('Precision: '+ str(sklearn.metrics.precision_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "print('Recall: '+ str(sklearn.metrics.recall_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "print('F-measure: '+ str(sklearn.metrics.f1_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "print('Precision-Recall AUC: '+ str(sklearn.metrics.average_precision_score(y_true=all_labels, y_score=probs2)))\n",
    "print('AUC: '+ str(sklearn.metrics.roc_auc_score(y_true=all_labels, y_score=probs2)))\n",
    "print('MCC: '+ str(sklearn.metrics.matthews_corrcoef(y_true=all_labels, y_pred=all_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BINARY\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "with torch.no_grad():\n",
    "    running_acc_test = 0\n",
    "    running_loss_test = 0\n",
    "    all_pred=[]\n",
    "    all_labels=[]\n",
    "    for batch in test_iterator:\n",
    "        batch.codes = batch.codes.to(device)\n",
    "        batch.label = batch.label.to(device)\n",
    "        output_test = model(batch.codes).squeeze(1)\n",
    "        loss_test = criterion(output_test,batch.label)\n",
    "        acc_test = binary_accuracy(output_test,batch.label)\n",
    "        running_acc_test += acc_test.item()\n",
    "        running_loss_test += loss_test.item()\n",
    "        all_pred += torch.round(output_test).tolist()\n",
    "        all_labels += batch.label.tolist()\n",
    "\n",
    "\n",
    "print('Test acc: ',running_acc_test/len(test_iterator))\n",
    "print('Test loss: ',running_loss_test/len(test_iterator))\n",
    "\n",
    "confusion = sklearn.metrics.confusion_matrix(y_true=all_labels, y_pred=all_pred)\n",
    "print('Confusion matrix: \\n',confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
